# Reading List

## The deep learning textbook

[_Goodfellow, Ian. et al., (2016). Deep Learning. MIT press._](https://www.deeplearningbook.org/)

This is the reference book for this module. The module will assume prior knowledge of basic machine learning, briefly covered in Part I of the book.  

## Basic machine learning textbooks
For the basic machine learning reference, with consistent mathematical notations to the _Deep Learning_ book.  
[_Bishop, C.M., (2006). Pattern Recognition and Machine Learning. Springer_](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

Other useful books that are freely available include:  
[_Hastie et al., The Elements of Statistical Learning. Springer_](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) 

[_MacKay, D., Information Theory, Inference, and Learning Algorithms._](http://www.inference.org.uk/mackay/itila/book.html)  

[_Barber, D., Bayesian Reasoning and Machine Learning. Cambridge University Press (2012)_](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/200620.pdf)


## Selected research papers and surveys

**The Deep Learning paper**
[LeCun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. nature, 521(7553), pp.436-444.](https://www.nature.com/articles/nature14539)  

**The CNN paper**
[LeCun, Y., Haffner, P., Bottou, L. and Bengio, Y., 1999. Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision (pp. 319-345). Springer, Berlin, Heidelberg.](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)

[Fukushima, K. and Miyake, S., 1982. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets (pp. 267-285). Springer, Berlin, Heidelberg.](https://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf)

**The AlexNet paper**
[Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, pp.1097-1105.](https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

**The ResNet paper**
[He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**The Dropout paper**
[Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), pp.1929-1958.](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_campaign=buffer&utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com)

**The BatchNorm paper**
[Ioffe, S. and Szegedy, C., 2015, June. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.](https://arxiv.org/pdf/1502.03167.pdf)

**The Adam paper**
[Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.](https://arxiv.org/pdf/1412.6980.pdf)

**The Attention paper**
[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å. and Polosukhin, I., 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

**The Vision Transformer Paper**
[Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.](https://arxiv.org/pdf/2010.11929.pdf)

**The GANs paper**
[Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Generative adversarial nets. Advances in neural information processing systems, 27.](https://papers.nips.cc/paper/5423-ge...al-nets.pdf)

**The DANN paper**
[Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M. and Lempitsky, V., 2016. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1), pp.2096-2030.](https://www.jmlr.org/papers/volume17/15-239/15-239.pdf)  

**The UNet paper**
[Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.](https://arxiv.org/pdf/1505.04597.pdf)
